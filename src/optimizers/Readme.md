#### Mathematical optimizers:
   - **Abdollahi (2015)**: Implementation of CHP optimal dispatch without heat grid dynamics from                                                                            [Optimization of energy production of a CHP plant with heat storage](https://ieeexplore-ieee-org.tudelft.idm.oclc.org/stamp/stamp.jsp?arnumber=7018636).
   - **Giraud (2017)**: Implementation of MILP method from [Giraud 2016](https://hal.archives-ouvertes.fr/tel-01687562/) and [Giraud 2017](https://www.researchgate.net/profile/R-Baviere-2/publication/315692284_Optimal_Control_of_District_Heating_Systems_using_Dynamic_Simulation_and_Mixed_Integer_Linear_Programming/links/58dbe18992851c611d0d2f7d/Optimal-Control-of-District-Heating-Systems-using-Dynamic-Simulation-and-Mixed-Integer-Linear-Programming.pdf).
   - **Li (2016)**: Implementation of non-linear mixed integer program with detailed model of heat grid dynamics from [Combined Heat and Power Dispatch ConsideringPipeline Energy Storage of District Heating Network](https://ieeexplore-ieee-org.tudelft.idm.oclc.org/document/7243359).
#### Reinforcement learning:
- Episodic, model-free reinforcement learning implementation. 
- **agent**: Reinforcement learning algorithm. By interacting with the environment agent transits to the next state and receives the reward. Based on the received reward Q-value function is updated. The next action is choosen based on the Q-value function and greedy policy.
- **state**: State of the agent consists of water chunks' temperatures according to the smallest discretization step of the inserted mass (e.g. if the discretization step of the inserted mass is 200000 [kg], and newly generated chunk has the mass 600000 [kg] of the temperature 90 [C], it is equal to three chunks of 90 [C] temperature), season, time of the day (morning, working hours, evening, night), heat demand, and electricity price one time-step in the future.
- **environment**: Environment in which reinforcement learning agent operates. Fully informative state is formed by receiving temperature at the inlet of the supply network pipe, and the mass flow from the simulator at every time step. This mimics real-world scenario where only one sensor at the inlet of the pipe (in the best case) is present, and possibility to reconstruct the information from the past observations. Based on these information, a new state is formed by pushing existing chunks in the pipe outside of it. The reward signal is combination of seven summed up and reshaped sub-reward signals: operation cost (cost for the heat and electricity production), electricity sale to the external grid, punishment for underdeliver and overdeliver heat (according to the heat demand), punishment for violating supply input temperature bounds, supply output temperature bounds (due to the heat loss through the network temperature at the outlet can drop below the allowed value, even if the input temperature was satisfactory), and punishment for violating maximum mass flow values. Hyperparameters (lower and upper bound, and the gradient) of these reward function define their trade-off. At the end of each episode agent's state is reseted.
- **parameters**: Data processing, physics parameters, district heating network configuration, reward function hyperparameters, state space representation.
